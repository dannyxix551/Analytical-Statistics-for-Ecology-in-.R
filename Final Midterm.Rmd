---
title: "Long Term Ecological Population and Productivity Statistical Analysis"
author: "Daniel Crownover"
date: "2023-02-23"
output:
  pdf_document:
    toc: true
    number_sections: true
    latex_engine: xelatex
always_allow_html: true
---



Answer the following questions in 1 knitted PDF document. Your answers do not need to be in paragraphs (they can be bullets). Also turn in your .Rmd file. 

## Problem 1 

a) Null and alternative hypotheses of your tests (2.5)

- H_0_1: The biomass between groups stayed the same between salt exposed and non salt exposed block groups

- H_a_1: The biomass between groups changed between salt exposed and non salt exposed block groops

b) Justification for your choice of test (2.5)
- ALL Figures "1.b"
- Our value is NOT statistically significant from shapiro wilks (p = .24) in figure 1.b Normality Assumption and Shaprio test. We fail/ do not have sufficient evidence to reject the null(that its normal) therefore, we assume normality. In the QQ plot(Figure 1.b.Normality and shapiro), as all the points fall approximately along the reference line, we can assume normality. 
In figure 1.b Homogeneity of Variance, the levine test showed, (p = .79) for block and (p = .69) for salt
Non-statistically significant value, we fail to reject null for block groups between biomass -- we assume their are equal variances. Using the Levene’s test for biomass between salt concentration, is not significant. Therefore, we can assume the homogeneity of variances in the different groups in biomass between salt
- Knowing that there is homogeneity among the variances for each, or pretty much the variances are equal, we are justified in our use of one way anova.



c) A description of how you checked the assumptions of your statistical test (10) and d) Results of your statistical test, interpreting your test in 2-3 sentences that include the appropriate reporting of the statistics as well as an appropriate figure (10)

- Figure 1.c Anova, The salt effect has a significant impact on the response variable. The F-value of 17.709 with a very small p-value (8.08e-06) indicates that the mean differences among salt levels are unlikely due to random chance. The generalized Eta Squared (ges) of 0.855 suggests that about 85.5% of the total variance in the response variable can be attributed to the salt effect.
- Figure 1.d line plot shows the line plot which shows salts negative effects on biomass.


e) An interpretation of any necessary post-hoc tests (5)

- Salt 10 to Salt 20: The change in the response variable from Salt 10 to Salt 20 is estimated to be approximately -8.15 units. This suggests that increasing the salt level from 10 to 20 results in a decrease in the response variable by about 8.15 units.(p = **** highly significant)

- Salt 10 to Salt 25: The change in the response variable from Salt 10 to Salt 25 is estimated to be approximately -6.675 units. This indicates that increasing the salt level from 10 to 25 results in a decrease in the response variable by about 6.675  units. (p = **** highly significant)

- Salt 10 to Salt 30: The change in the response variable from Salt 10 to Salt 30 is estimated to be approximately -11.075 units. This suggests that increasing the salt level from 10 to 30 results in a decrease in the response variable by about 11.075 units.(p = **** highly significant)

- Salt 10 to Salt 35: The change in the response variable from Salt 10 to Salt 35 is estimated to be approximately -8.375 units. This indicates that increasing the salt level from 10 to 35 results in a decrease in the response variable by about 8.375 units. (p = **** highly significant)

- Salt 15 to Salt 20: The change in the response variable from Salt 15 to Salt 20 is estimated to be approximately -12.325 units. This suggests that increasing the salt level from 15 to 20 results in a decrease in the response variable by about 12.325 units. (p = **** highly significant)

- Salt 15 to Salt 25: The change in the response variable from Salt 15 to Salt 25 is estimated to be approximately -10.85 units. This indicates that increasing the salt level from 15 to 25 results in a decrease in the response variable by about 10.85 units. (p = **** highly significant)

- Salt 15 to Salt 30: The change in the response variable from Salt 15 to Salt 30 is estimated to be approximately -15.25 units. This suggests that increasing the salt level from 15 to 30 results in a decrease in the response variable by about 15.25 units. (p = **** highly significant)

- Salt 15 to Salt 35: The change in the response variable from Salt 15 to Salt 35 is estimated to be approximately -12.55 units. This indicates that increasing the salt level from 15 to 35 results in a decrease in the response variable by about 12.55 units. (p = **** highly significant)

kyle don't take points off for not reporting the p-value, you can just glace at them in the figure. I described each one and why
since we are only looking at salt we don't care about block... although I added it in for good measure

## Problem 2 

a) Null and alternative hypotheses of your tests (2.5)

H_0: There is no significant effect of the dose of supplements or the type of supplement (Vitamin B and Zinc) on the thickness of pangolin scales

H_1: There is a significant effect of the dose of supplements or the type of supplement on the thickness of pangolin scales

b) Justification for your choice of test (2.5)
ALL Figures labled "2.b"
Figure 2.b Normality Assumption, shapiro test(p = .669) not significant, passes and assume normality. figure 2.b homogeneity of variance, the levene test shows (p = .14) which is not stat significant so we can assume  normality of variances and are justified to continue with two way anova.

Figure 2.b and c 

Interpretation: The interaction between dose and supplement type also shows a significant impact on scale thickness. The F-value of 4.107 with a small p-value (2.20e-02) suggests that the joint effect of dose and supplement type on scale thickness is significant. The generalized Eta Squared (ges) of 0.132 indicates that about 13.2% of the total variance in scale thickness can be attributed to the interaction between dose and supplement type. 77% of the variance in scale thickness can be attributed to dose and 22% can be attributed to dose.


c) A description of how you checked the assumptions of your statistical test (10)

- I also answered with some questions in "b" part of this question

I chose multiple comparison and simple main effects because if you have met the assumptions of the two-way ANOVA (e.g., homogeneity of variances), it is better to use the overall error term (from the two-way ANOVA) as input in the one-way ANOVA model. This will make it easier to detect any statistically significant differences if they exist (Keppel & Wickens, 2004; Maxwell & Delaney, 2004).

d) Results of your statistical test, interpreting your test in 2-3 sentences that include the appropriate reporting of the statistics as well as an appropriate figure (10)

Result of Anova Figure 2.d Anova - The two-way ANOVA conducted on pangolin scales thickness revealed significant main effects for both dose (F(2, 54) = 92.000, p < .001, η² = 0.773) and supplement type (F(1, 54) = 15.572, p = .000231, η² = 0.224). Additionally, a significant interaction effect was observed between dose and supplement type (F(2, 54) = 4.107, p = .022, η² = 0.132), indicating that the relationship between dose and scales thickness varied depending on the supplement type administered. The asterisks (*) denote statistical significance at the p < .05 level. the output suggests that both the "dose" and "supp" factors have significant effects on pangolin scales thickness, and there is also a significant interaction effect between these two factors. 

Figure 2.d Simple Main Effect
- The effect of the Vitamin B supplement on scale thickness is highly significant. The F-value of 62.542 with an extremely small p-value (8.75e-15) suggests that the mean differences in scale thickness across different doses of the Vitamin B supplement are highly unlikely to occur due to random chance. The generalized Eta Squared (ges) of 0.698 indicates that about 69.8% of the total variance in scale thickness can be attributed to the effect of the Vitamin B supplement.
- Similarly, the effect of the Zinc supplement on scale thickness is highly significant. The F-value of 33.565 with an extremely small p-value (3.36e-10) indicates that the mean differences in scale thickness across different doses of the Zinc supplement are highly unlikely to occur due to random chance. The generalized Eta Squared (ges) of 0.554 suggests that about 55.4% of the total variance in scale thickness can be attributed to the effect of the Zinc supplement.

2.d Multiple Comparison Test

- There is a significant difference in scale thickness between groups receiving 0.5 and 1 doses of the Vitamin B supplement. The estimated difference is 8.79 units, with a confidence interval between approximately 4.90 and 12.68 units. The p-value of 1.75e-05 indicates that this difference is highly unlikely to have occurred by chance alone
- there is a significant difference in scale thickness between groups receiving 0.5 and 2 doses of the Vitamin B supplement. The estimated difference is 18.16 units, with a confidence interval between approximately 14.27 and 22.05 units. The extremely small p-value of 1.66e-11 indicates the highly significant nature of this difference.
- There is a significant difference in scale thickness between groups receiving 1 and 2 doses of the Vitamin B supplement. The estimated difference is 9.37 units, with a confidence interval between approximately 5.48 and 13.26 units. The p-value of 6.61e-06 indicates high significance.
- There is a significant difference in scale thickness between groups receiving Dose 1 and Dose 2 of the Vitamin B supplement. The estimated difference is 9.37 units, with a confidence interval between approximately 5.48 and 13.26 units. The p-value of 6.61e-06 indicates that this difference is highly unlikely to have occurred by chance alone.
- There is a significant difference in scale thickness between groups receiving Dose 0.5 and Dose 1 of the Zinc supplement. The estimated difference is 9.47 units, with a confidence interval between approximately 5.31 and 13.63 units. The p-value of 1.58e-05 indicates high significance.


e) An interpretation of any necessary post-hoc tests (5)

- did not run post-hoc

## Problem 3 


 (a) specify your hypotheses (5) 
 
H_0: There is no difference in website loading times of the two providers
H_a: There is a difference in the website loading times of the two providers

 (b) check that the data meet the assumptions of the statistical test you plan to use (10)
- Figure 3.b Normality by Groups and Shapiro Wilks
Both fail test shapiro wilks (p = .015) and (p = .0019) and pass equality of variance so we continue with parametric t test. The Levene’s test is not significant (p > 0.05m, p = 0.0719). Therefore, we can assume the homogeneity of variances in the different groups.
Continue with the parametric t test and we are assuming that the homogeneity of variance are equal.

Figure 3 Parametric t test (p = .18) gives non significant p value






 (c) Test your hypotheses. If your data do not meet the assumptions, test your hypotheses both by transforming the data and by using a nonparametric approach (10)
See all figures with 3.c

 (d) Explain the results of your statistical tests. (10)

- See figure 3.d


## Problem 4 

(a) Hypothesize a relationship between your variables. Include the null and alternative hypothesis. (5)

Null hypothesis (H0): There is no relationship between GDP per capita (GDPpc) and the Yale Environmental Performance Index (EPI2018Score).

Alternative hypothesis (H1): There is a relationship between GDP per capita (GDPpc) and the Yale Environmental Performance Index (EPI2018Score).


(b) Calculate the correlation coefficient between the two variables. Interpret. (5)

Figure 4.b and c GGpairs and 4.b and c Coorelation

 - .69  coefficient could be higher, more testing and transformation needed






(c) Develop and discuss the scatterplots between the two variables. Does the relationship look linear or linear-log (or something else)? Should you transform one or both variables? Include your scatterplots (original, transformed) and discussion.(10)

Linear Model 1 (Figure 4.c linear model 1 and 4.c plot linear model 1) show extensive skewness and kurtosis on GDPpc
Figure 4.b and c Scatterplot, shows we only need to transform GDPpc because this has the largest kurtosis and skewness.
 4.c Non-constant Error Variance Test Linear Model 1 [raw data with outliers] - P values (p = 4.6154e-12, 2.61e-17 and 2.83e-10) highly stat significant indicating heteroskedascitcity
the results of both tests indicate that there is significant heteroscedasticity in the regression model, suggesting that the assumption of constant variance is violated. This implies that the variability of the residuals changes across different levels of GDP per capita, which could potentially affect the reliability of the regression model's estimates and inference. 
Transformed with figure 4.c log data

(d) Perform a simple linear regression analysis on the two variables (after transformation, if needed). Make sure that your x and y variables are correct. Write the model equation and interpret the coefficient for the slope (10)

Created simple linear model with log transformed data(Figure 4.d Linear Model Logged Data) 
Created equation, figure 4.d equation for slope of logged data 

(e) Interpret p-value of the slope coefficient in terms of hypothesis above. (5)

Figure 4.e interpertation for equation above
for every one percent  change in GDP per capita the EPISCORE increases by 0.086 units.


  
(f) Interpret the R2 value (5)
Shown in Figure
Value is 0.6657, which means that approximately 66.57% of the variance in the Environmental Performance Index (EPI) can be explained by the variation in the log of GDP per capita (GDPpc).


## Problem 5 
 (a) Null and alternative hypotheses of your model (5)
  
H_0: There is no significant effect of the predictor weather variables (temperature, solar radiation, wind speed) on ozone concentration or their corresponding interaction effects.

H_1: There is a significant effect of at least one weather variable on ozone concentration.

  (b) Results of your statistical test, interpreting the fit of the selected model in 2-3 sentences that include the appropriate reporting of the statistics in a table (20)
Figure 5.b first linear model  and Figure 5.b VIF, our VIF all are under pass and pass test for multicollinearity, we will continue with cross validation
Figure 5.b GVLMA assumptions fail




  
  (c) An interpretation of the regression model coefficients (i.e., what do each of the main effect(s) and interaction effect(s) mean (10)

See figure 5.c exponentiated values of final linear model: 
  
  (d) A description of how you checked the assumptions of your statistical test (10)

Every time temperature increases by one unit ozone increases by 5.9%. Everytime wind increases by one unit ozone increases 5.64 %. 



Interaction effects Temp:Wind: the relationship between temperature and ozone concentration is modified by wind speed. A negative coefficient (-0.1500873) indicates that the effect of temperature on ozone concentration decreases as wind speed increases, or vice versa. In other words, the impact of temperature on ozone concentration is tempered by wind speed.
Every additional rise in unit temp decreases the positive/neg effect of wind on the dependent variable of ozone by -.15%

Interaction effect temp:rad: Similarly, this interaction effect suggests that the relationship between temperature and ozone concentration is influenced by radiation levels. A positive coefficient (0.0032171) indicates that the effect of temperature on ozone concentration increases with higher radiation levels, or vice versa. In other words, the impact of temperature on ozone concentration is enhanced by radiation levels.
every additional in temp decreases the positive/neg effect of solar radiation on dependent variable of ozone by -0.00321%.



Steps for cross validation:
From the cross validation and subsequent data wrangling we have determined several things:
- Running the cross validation with the interactive effects on the logged ozone we were able to obtain more accurate results from the CV, with that and after removing outliers from that data frame, all assumptions passed in the GVLMA.

From these results we determine that the most accurate predictor variables are temp and wind with the most prominent interaction effects being temp:wind and temp:rad on the response variable of ozone.

(e) Graphs that depict (i) the relationship between the independent variable and dependent variable when a variable is only included as a main effect, (ii) the relationship between the interacting independent variables with the dependent variable (10)

Figure 5.e simple main effects 
Figure 5.e plot coefficients simple main effects and interaction effect plots
Figure 5.e Interaction plots for interaction effects




## Problem 6 


a)	What is the research question underpinning the study (10 points)

The main research question of the study is to identify the environmental factors that influence the distribution of coral reef fish species on a regional scale. The study aims to determine which remotely measured environmental variables are most influential in determining the distributions of coral reef fish species. This research is important for managing coral reef species effectively, as understanding the factors affecting their distribution can inform conservation and management efforts.

b)	What type of model did the authors choose to use and why? (5 points)

The authors chose to use Generalized Linear Models (GLMs) for their species distribution models. GLMs are a statistical modeling framework that allows for the analysis of data with non-normal distributions, such as binomial data (presence/absence data in this case). There are several reasons why GLMs were chosen:

1.Binomial data: GLMs are well-suited for modeling binary response variables, which is appropriate for presence/absence data in this study.

2. Flexibility of GLMs offer flexibility in modeling complex ecological relationships realistically. They allow for the incorporation of multiple predictor variables to explain species distributions.

3. Strong statistical foundation: GLMs are based on a strong statistical foundation, making them reliable for analyzing ecological data.

4. Ease of interpretation: GLMs provide interpretable results, making it easier to understand the relationship between predictor variables and species distributions.

5. Transferability to GIS: Predictions resulting from GLMs can be easily transferred into Geographic Information Systems (GIS) for mapping, which is important for visualizing and analyzing spatial patterns of species distributions.





c)	Describe the data that the authors used. What is the response variable? Explanatory variables? (10 points)


The authors used data collected from coral reef sites in Kimbe Bay, Papua New Guinea from the years 1994, 2001 and 2002

The response variable in their analysis is the presence or absence of each fish species at each surveyed site. This binary response variable indicates whether a particular species was observed at a given site during timed-swim visual observations.

The explanatory variables, also known as predictor variables, are the environmental factors that the authors hypothesized may influence the distribution of coral reef fish species. These variables were remotely measured and included:

Depth: The depth of the water at each surveyed site.
Presence of a land-sea interface: This variable likely indicates the proximity of the reef site to the shoreline, potentially affecting factors such as water temperature and nutrient availability.
Exposure: This variable refers to the degree of exposure of the reef site to prevailing wind directions.
Distance to the nearest estuary: This variable measures the distance from each reef site to the nearest river estuary, which may influence water quality and sedimentation levels.
These explanatory variables were chosen based on their potential ecological significance and their availability for remote measurement, which is essential for broad-scale conservation and management applications.



As they tested for these, they only found these important:

ocean charts, remote sensing and local expert knowledge



d)	Interpret the equation and results from table 4 for Caesio lunaris. (10 points)


Here's the breakdown of the equation:

- ln(p/(1 - p)): This is the natural logarithm of the odds ratio of the probability of occurrence of Caesio lunaris. In logistic regression, the log odds of an event occurring are modeled as a linear combination of the predictor variables.

- -1.086: This is the intercept term of the equation. It represents the log odds of occurrence when all predictor variables are zero. In this case, it suggests the baseline log odds of occurrence for Caesio lunaris when Exposure and Depth are zero.

- 2.6 × Exposure: Exposure is one of the predictor variables. The coefficient 2.6 indicates the change in the log odds of occurrence of Caesio lunaris for a one-unit increase in Exposure, holding all other variables constant. A positive coefficient suggests that as Exposure increases, the log odds of occurrence of Caesio lunaris also increase.

- 0.003 × Depth: Depth is another predictor variable. The coefficient 0.003 indicates the change in the log odds of occurrence of Caesio lunaris for a one-unit increase in Depth, holding all other variables constant. Here, a positive coefficient suggests that as Depth increases, the log odds of occurrence of Caesio lunaris also increase, but to a lesser extent compared to Exposure.

In summary, the equation suggests that the probability of occurrence of Caesio lunaris is influenced by both Exposure and Depth. An increase in Exposure and Depth leads to an increase in the log odds of occurrence of Caesio lunaris, with Exposure having a stronger effect than Depth.



e) How did the authors compare models? (5 points)

The authors compared models using the Akaike Information Criterion (AIC). The AIC is a measure of the relative quality of a statistical model for a given set of data. It takes into account both the goodness of fit of the model and the complexity of the model, penalizing the latter to avoid overfitting.

In their study, the authors developed logistic regression models for presence/absence data of 227 fish species. They then tested the efficiency of these models by comparing their AIC values to those of hypothetical species with random distributions. By comparing the AIC values of real species with the AIC values of hypothetical species distributions, they were able to identify efficient models.

Efficiency in this context refers to how well the model describes the observed data while avoiding overfitting. Models with lower AIC values are considered to be more efficient, indicating a better balance between goodness of fit and model complexity.

The authors found efficient models for 118 species, most of which were highly habitat-specific. This comparison allowed them to identify the most effective models for predicting the distribution of coral reef fish species based on the selected predictor variables.


f)	Discuss two of the limitations the authors identify in their analysis? Discuss one potential limitation they do not bring up. (10 points)





Two limitations identified by the authors in their analysis are:

1. Data Uncertainty: The authors acknowledge that the accuracy of the environmental and habitat data used in their study may vary. While they used local-scale data layers for Kimbe Bay from various sources, including printed marine charts, expert consultation, GIS analysis, and multivariate bioregionalization of coral data, they did not explicitly consider the uncertainty associated with these data layers. For example, exposure data were classified based on expert opinion rather than directly measured wave energy, which could introduce inaccuracies. Similarly, bathymetry data were obtained from remote imagery with coarse resolution, which may not accurately represent the underwater terrain. The authors note that future studies may require in-situ measurements to obtain more accurate exposure and bathymetry data.

2. Model Generalization: Another limitation mentioned by the authors is the challenge of applying their species distribution models to other regions or scales. The environmental and habitat data used in their study were specific to Kimbe Bay, and the models may not generalize well to other locations with different environmental conditions or spatial scales. For example, while the models were effective for predicting the distribution of coral reef fish species in Kimbe Bay, they may not be applicable to regions with different reef structures or anthropogenic impacts. This limitation highlights the need for caution when extrapolating the results of localized studies to broader spatial scales or different geographic regions.

One potential limitation that the authors do not explicitly address in their analysis is the assumption of spatial independence in the presence/absence data used to fit the logistic regression models. Spatial autocorrelation, where the presence or absence of a species at one site may be correlated with nearby sites due to shared environmental characteristics or dispersal processes, could violate this assumption. Ignoring spatial autocorrelation may lead to biased parameter estimates and inflated type I error rates. Therefore, future studies could benefit from incorporating spatial modeling techniques to account for potential spatial autocorrelation in the data.







```{r, echo=FALSE}
knitr::opts_chunk$set( warning=FALSE, tab.cap = NULL, fig.show='only', echo=FALSE, results='asis', message=FALSE, fig.align='center')
```

```{r}
library(ggplot2)
library(interactions)
library(car)
library(lmtest)
library(ggplot2)
library(dplyr)
library(tidyr)
library(rmarkdown)
library(GGally)
library(gvlma)
library(rstatix)
library(tidyverse)
library(dplyr)    # a set of packages for data manipulation and management
library(caret)    # package for classification and regression, Machine Learning
library(lattice) 
library(lmtest)   # lmtest contains a bunch of linear model tests, as well as the likelihood ratio test that we will use here


theme_Publication <- function(base_size=10, base_family="Arial") {
  
  (theme_foundation(base_size=base_size, base_family=base_family)
   + theme(plot.title = element_text(hjust = 0.5),
           text = element_text(),
           panel.background = element_rect(colour = NA),
           plot.background = element_rect(colour = NA),
           panel.border = element_rect(colour = NA),
           axis.title = element_text(size = rel(1)),
           axis.title.y = element_text(angle=90,vjust =2),
           axis.text = element_text(), 
           axis.line = element_line(colour="black"),
           axis.ticks = element_line(),
           panel.grid.major = element_line(colour="#f0f0f0"),
           panel.grid.minor = element_blank(),
           legend.key = element_rect(colour = NA),
           legend.position = "right",
           legend.title = element_text(face="italic"),
           strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0")
   ))
  
}

```
```{r chunk_name1, echo=FALSE, warning=FALSE, fig.show='only', include=FALSE}
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("ggplot2", "ggpubr", "rstatix", "dplyr", "ggthemes", "ggformula", "ggpmisc", "GGally", "arm")
ipak(packages)

```



#Problem 1 - 30
```{r}
 saltdat <- read.csv("/Users/daniel/Downloads/BIO 591 Coding Files/Midterm/Midterm/salt.csv")
```

## 1.a H0 State H_0 and H_a

H_0_1: The biomass between groups stayed the same between salt exposed and non salt exposed block grouops

H_a_1: The biomass between groups changed between salt exposed and non salt exposed block groups



## 1.b Get summary statistics and visualize
```{r}
library(kableExtra)
saltdat %>%
  group_by(salt) %>%
  get_summary_stats(biomass, type = "mean_sd") %>%
  kbl() %>%
  kable_styling()

```


## 1.b Identify outliers in each block
```{r}
saltdat %>%
  group_by(salt) %>%
  identify_outliers(biomass) %>%
  kbl() %>%
  kable_styling()

```
failed the outlier test, if other test are stat significant, take outlier out
## 1.b Boxplot
```{r}
library(ggpubr)
ggboxplot(saltdat, x = "block", y = "biomass") +
ggtitle("biomass vs block ")
ggboxplot(saltdat, x = "salt", y = "biomass") +
ggtitle("biomass vs salt")

```



## 1.b Normality Assumption and Shapriro Test
```{r chunk_name17, warning=FALSE, fig.show='only'}
# Build the linear model

library(ggplot2)
library(car)
model2  <- lm(biomass ~ block + salt,
             data = saltdat)



# Create a QQ plot of residuals
ggqqplot(residuals(model2))

# Conduct a Shapiro-Wilk test for normality of residuals
shapiro_test(residuals(model2)) %>%
  kbl() %>%
  kable_styling()

```
answer to 1b. 

Our value is NOT statistically significant from shapiro wilks (p = .24), we fail/ do not have sufficient evidence to reject the null(that its normal) therefore, we assume normality. In the QQ plot, as all the points fall approximately along the reference line, we can assume normality. 

## 1.b Homogneity of variances 
```{r chunk_name18, warning=FALSE, fig.show='only'}
library(rstatix)
## they both went down so we will NOT remove any residuals

## essentially only one treatment occured per block group,which is why we are not comparing for each block group...



saltdat %>% 
  mutate(block = as.factor(block)) %>% 
  levene_test(biomass ~ block) %>%
  kbl() %>%
  kable_styling()

saltdat %>% 
  mutate(salt = as.factor(salt)) %>% 
  levene_test(biomass ~ salt) %>%
  kbl() %>%
  kable_styling()
```

## 1.c Compute One Way ANVOA
```{r}

library(kableExtra)

res.aov <- saltdat %>%  mutate(salt = as.factor(salt)) %>%    mutate(block = as.factor(block)) %>% 

 anova_test(biomass ~ salt + block, detailed = T)
res.aov

#this is how you do it in base R
summary(aov(biomass ~ salt, data = saltdat)) 

```

## 1 c d and e Post Hoc Test:
```{r}
library(report)
# Pairwise comparisons
library(rstatix)

saltdat$salt <- as.factor(saltdat$salt)
saltdat$block <- as.factor(saltdat$block)

# Perform Tukey's HSD test for pairwise comparisons
pwc <- saltdat %>% 
  tukey_hsd(biomass ~ salt + block)

# Print the pairwise comparisons
print(pwc)  %>%
  kbl() %>%
  kable_styling()

```

## 1.d line plot
```{r}

library(ggplot2)

saltdat %>%
  ggplot(aes(x = salt, y = biomass, group = block, color = block)) +
  geom_line()
```






## Problem 2
```{r}
Pangolindat <- read.csv("/Users/daniel/Downloads/BIO 591 Coding Files/Midterm/Midterm/ScaleThickness.csv")
```

## 2.b Summary Statistics
```{r, warning=FALSE, fig.show='only'}

# as "D0.5", "D1", "D2"
Pangolindat$dose <- factor(Pangolindat$dose)
Pangolindat$supp <- factor(Pangolindat$supp)

Pangolindat %>%
  group_by(dose, supp) %>%
  get_summary_stats(thick, type = "mean_sd")  %>%
  kbl() %>%
  kable_styling()

```
## 2.b Identify outliers in each block
```{r, warning=FALSE, fig.show='only'}


Pangolindat %>%
  group_by(dose, supp) %>%
  identify_outliers(thick)  %>%
  kbl() %>%
  kable_styling()
```
failed the outlier test, if other test are stat significant, take outlier out
## 2 boxplot
```{r}
library(ggpubr)
library(gridExtra)
library(ggplot2)
# Create the first boxplot
boxplot1 <- ggboxplot(Pangolindat, x = "supp", y = "thick") +
  ggtitle("Scale Thickness vs Supplement Type")

# Create the second boxplot
boxplot2 <- ggboxplot(Pangolindat, x = "dose", y = "thick") +
  ggtitle("Scale Thickness vs Dose Concentration")

# Arrange and display both plots


```

## Plotting the Data
```{r}
Pangolindat %>%
  group_by(supp, dose) %>%
  get_summary_stats(thick, type = "mean_sd")

bxp <- ggboxplot(Pangolindat, x = "dose", y = "thick", color = "supp",
          palette = c("#00AFBB", "#E7B800"))
bxp
```

## 2.b Normality Assumption
```{r}
# Build the linear model
model3  <- lm(thick ~ dose*supp,
             data = Pangolindat)
# Create a QQ plot of residuals
ggqqplot(residuals(model3))

shapiro_test(residuals(model3))  %>%
  kbl() %>%
  kable_styling()
```

## 2.b Homogneity of Variance
```{r}

Pangolindat %>%
  mutate(dose = as.factor(dose)) %>%
  levene_test(thick ~ dose * supp, data = .)  %>%
  kbl() %>%
  kable_styling()


```

## 2.d Compute two way anova
```{r}

res.aov3 <- Pangolindat %>% anova_test(thick ~ dose*supp)
res.aov3  %>%
  kbl() %>%
  kable_styling()

```

## 2.d Simple Main Effect
```{r}
# Group the data by gender and fit  anova
model <- lm(thick ~ dose * supp, data = Pangolindat)
Pangolindat %>%
  group_by(supp) %>%
  anova_test(thick ~ dose, error = model)  %>%
  kbl() %>%
  kable_styling()

```

## 2.d Multiple Comparison Test
```{r}
library(emmeans)
pwc <- Pangolindat %>%  
  group_by(supp) %>%
  tukey_hsd(thick ~ dose) 
pwc  %>%
  kbl() %>%
  kable_styling()
```
Interpretation: There is a significant difference in scale thickness between groups receiving 0.5 and 1 doses of the Vitamin B supplement. The estimated difference is 8.79 units, with a confidence interval between approximately 4.90 and 12.68 units. The p-value of 1.75e-05 indicates that this difference is highly unlikely to have occurred by chance alone
Interpretation: Similarly, there is a significant difference in scale thickness between groups receiving 0.5 and 2 doses of the Vitamin B supplement. The estimated difference is 18.16 units, with a confidence interval between approximately 14.27 and 22.05 units. The extremely small p-value of 1.66e-11 indicates the highly significant nature of this difference.
nterpretation: There is a significant difference in scale thickness between groups receiving 1 and 2 doses of the Vitamin B supplement. The estimated difference is 9.37 units, with a confidence interval between approximately 5.48 and 13.26 units. The p-value of 6.61e-06 indicates high significance.
Iterpretation: There is a significant difference in scale thickness between groups receiving Dose 1 and Dose 2 of the Vitamin B supplement. The estimated difference is 9.37 units, with a confidence interval between approximately 5.48 and 13.26 units. The p-value of 6.61e-06 indicates that this difference is highly unlikely to have occurred by chance alone.
Interpretation: There is a significant difference in scale thickness between groups receiving Dose 0.5 and Dose 1 of the Zinc supplement. The estimated difference is 9.47 units, with a confidence interval between approximately 5.31 and 13.63 units. The p-value of 1.58e-05 indicates high significance.
















## 2 tukey post hoc test
```{r}

Pangolindat %>%
tukey_hsd(thick ~ supp*dose)  %>%
  kbl() %>%
  kable_styling()

```


## 2 Visualization: box plots with p-values
```{r}
# Visualization: box plots with p-values
library(ggpubr)

# Convert "supp" to a factor if it's not already
Pangolindat$supp <- as.factor(Pangolindat$supp)

# Add xy position
pwc <- pwc %>%
  add_xy_position(x = "supp")

# Plot boxplot
bxp <- ggboxplot(Pangolindat, x = "supp", y = "thick")

# Add manual p-values
bxp + 
  stat_pvalue_manual(pwc) +
  labs(
    subtitle = get_test_label(res.aov3, detailed = TRUE),
    caption = get_pwc_label(pwc)
  )

```



## Problem 3
```{r}
Internetdat <- read.csv("/Users/daniel/Downloads/BIO 591 Coding Files/Midterm/Midterm/Internet.csv")

```

```{r}
Internetdat2 <- data.frame(id = 1:40, 
                     group = rep(c("turbo.net", "speed.web"), each = 20), 
                     speed = c(0.22807330
,0.06620022,0.83045692,0.44223432,0.22929000,0.16215080,0.03356102,0.06993777,0.10012863,0.08330796,1.56918512,0.37560128,0.98659006,0.39338981,1.00922281,0.06233554,0.34715016, 0.36087963,0.94328549,0.02653665,0.18757695,0.07417681,0.49443755,0.30822151,0.18832694,0.14523221,0.04456563,0.07729608,0.10116780,0.08813298,0.79685491,0.27268981,0.56263425,0.28231955,0.57228704,0.07090477,0.25704647,0.26463385,0.54400887,0.03736872))
```


## 3 Summary Stats
```{r}
library(dplyr)

Internetdat2 %>%
  group_by(group) %>%
  get_summary_stats(speed, type = "mean_sd",)  %>%
  kbl() %>%
  kable_styling()
```
## 3 Boxplot Data
```{r}
bxp <- ggboxplot(
  Internetdat2, x = "group", y = "speed", 
  ylab = "Speeds", xlab = "Groups", add = "jitter"
  )
bxp

```

## 3 Identify outliers by groups
```{r}
Internetdat2 %>%
  group_by(group) %>%
  identify_outliers(speed)  %>%
  kbl() %>%
  kable_styling()
## Decide wether to take these out
```
There were no extreme outliers.


## 3.b Normality by Groups and Shapiro Wilks
```{r}
## pick 123 of what --> bsb_df2 variables -->grouping ---> shapiro test
set.seed(123)
Internetdat2 %>%
  group_by(group) %>%
  shapiro_test(speed)  %>%
  kbl() %>%
  kable_styling()

# Draw a qq plot by group
ggqqplot(Internetdat2, x = "speed", facet.by = "group") 

```
failed both test(p = .015) for speedweb,(p=0.0019) for turbonet - need to log

## 3.b Equality of Variances
```{r}
Internetdat2 %>% levene_test(speed ~ group)  %>%
  kbl() %>%
  kable_styling()
```
- pass so we continue with parametric (p = 0.07)

## 3 Parametric t test regular data 
```{r}
stat.test2 <- Internetdat2 %>% 
  t_test(speed ~ group, var.equal = FALSE, detailed = T) %>%
  add_significance()
stat.test2  %>%
  kbl() %>%
  kable_styling()
```



## 3 Effect Size
```{r}
Internetdat2 %>%  cohens_d(speed ~ group, var.equal = FALSE)  %>%
  kbl() %>%
  kable_styling()
```


## 3 Report results parametric t test
```{r}
stat.test2 <- stat.test2 %>% add_xy_position(x = "group")
bxp + 
  stat_pvalue_manual(stat.test2, tip.length = 0) +
  labs(subtitle = get_test_label(stat.test2, detailed = TRUE))
```

## 3.c Log Transforming Data (1) 
```{r}
Internetdat2$log.speed1<-log(Internetdat2$speed)
```

## 3.c Levine test log data
```{r chunk_name49, echo=FALSE, warning=FALSE, fig.show='only'}
library(rstatix)
library(kableExtra)

# Perform Levene's test and print the result using kableExtra
Internetdat2 %>% 
  levene_test(log.speed1 ~ group) %>%
  kbl() %>%
  kable_styling()

```


pass(p = .18)

## 3.c Shapiro Test Logged
```{r}
## pick 123 of what --> bsb_df2 variables -->grouping ---> shapiro test
set.seed(123)
Internetdat2 %>%
  group_by(group) %>%
  shapiro_test(log.speed1)  %>%
  kbl() %>%
  kable_styling()

# Draw a qq plot by group
ggqqplot(Internetdat2, x = "log.speed1", facet.by = "group")
```

pass(p>.05) speed web(p=.4), turbonet(p-.4) pass and pass

variances are equal we conduct parametric t-test



paired t test

## 3.c Parametric t test logged
```{r}
stat.test3 <- Internetdat2 %>% 
  t_test(log.speed1 ~ group, var.equal = FALSE, detailed = T) %>%
  add_significance()
stat.test3  %>%
  kbl() %>%
  kable_styling()
```
## Confidence Interval
```{r}
# Assuming "Internetdat2" is your dataframe
stat.test4 <- Internetdat2 %>%
  group_by(group) %>%
  summarize(conf_interval = t.test(log.speed1)$conf.int)

stat.test4  %>%
  kbl() %>%
  kable_styling()

```
## 3.c Effect Size logged
```{r}
Internetdat2 %>%  cohens_d(log.speed1 ~ group, var.equal = FALSE)  %>%
  kbl() %>%
  kable_styling()
```
pass effect size is negligible
## 3.c Non-parametric Logged wilcoxin test
```{r}
stat.testwilx <- Internetdat2 %>% wilcox_test(log.speed1 ~ group) 
stat.testwilx  %>%
  kbl() %>%
  kable_styling()
```
pass but we aren't supposed to do this with our data

## 3.c Effect Size non parametric

```{r}
Internetdat2 %>%  wilcox_effsize(log.speed1 ~ group)  %>%
  kbl() %>%
  kable_styling()
```

## 3.c  Report Results Wilcoxin Test
```{r}
bxp + 
  labs(subtitle = get_test_label(stat.testwilx, detailed = TRUE))
```


## 3.d report results
the confidence intervals for the parametric t-test of the logged data are 
speed.web	-2.0899827			
speed.web	-1.2400592			
turbo.net	-2.0333104			
turbo.net	-0.9000789	

the effect size is negligible and the p value is non significant(p=.56) which suggest that we fail to reject the null for the logged data and that the difference between the loading speed between turbo net and speed web does not change.






























## Problem 4

```{r}
yaledat <- read.csv("/Users/daniel/Downloads/BIO 591 Coding Files/Midterm/Midterm/yaleEPI2018.csv")

```

## 4.b and c GGpairs
```{r}
library(GGally)
theme_set(theme_bw())
ggpairs(yaledat[c(9,11)])
```
- coorelation coeficcient .699 but data is heavily skewed and needs to be log transformed
## 4.b and c Coorelation
```{r}
cor1 <- yaledat %>% cor_test(GDPpc,EPI2018Score)
cor1

cor(yaledat$GDPpc, yaledat$EPI2018Score)  %>%
  kbl() %>%
  kable_styling()
```

coorelation could be closer
##  4.b and c Scatterplot
```{r}
yaledat %>% ggplot(aes(x = GDPpc, y = EPI2018Score)) + geom_point()
```
scatterplot shows real heteroskedasticity

## 4.c Making Linear Model 1 [raw data with outliers]
```{r}

# linear regression model
lm1 <- lm(`EPI2018Score` ~ `GDPpc`, data = yaledat)

# Generate summary table
sum_lm1 <- summary(lm1)

# Extract coefficients table
coefficients_table <- sum_lm1$coefficients

# Load required library
library(knitr)

# Create a pretty table using kable
kable(coefficients_table, format = "markdown")

```

- The coefficient estimate for GDP per capita is 0.0004678, indicating that for every one-unit increase in GDP per capita, the EPI2018Score is expected to increase by approximately 0.0004678 units, holding other variables constant. Both the intercept and the coefficient for GDP per capita have extremely small p-values (close to zero), suggesting strong evidence against the null hypothesis that their coefficients are zero, indicating that both variables are statistically significant predictors of the Yale Environmental Performance Index. The t-values for both the intercept and GDP per capita are also notably high, indicating that their estimates are significantly different from zero, reinforcing their significance in predicting the EPI2018Score.



## 4.c Calculating prediction and confidence intervals
```{r}

conf <- as.data.frame(predict(lm1, newdata = yaledat,
                             interval = "confidence",
                             level = 0.95))

conf$'EPI2018Score' <- yaledat$'EPI2018Score'

pred <- as.data.frame(predict(lm1, newdata = yaledat,
                             interval = "prediction",
                             level = 0.95))

pred$'EPI2018Score' <- yaledat$'EPI2018Score'

```

## 4.c Plot Linear Model 1 [raw data with outliers]
```{r}
plot(lm1)
```
You can clearly see from these plots the skewness of the data, something that only a log transformation

## 4.c Non-constant Error Variance Test Linear Model 1 [raw data with outliers]
```{r chunk_name8, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
library(car)
library(lmtest)
ncvTest(lm1) 
bptest(lm1, studentize = F)
bptest(lm1, studentize = T) 
```
P values (p = 4.6154e-12, 2.61e-17 and 2.83e-10) highly stat significant indicating heteroskedascitcity
the results of both tests indicate that there is significant heteroscedasticity in the regression model, suggesting that the assumption of constant variance is violated. This implies that the variability of the residuals changes across different levels of GDP per capita, which could potentially affect the reliability of the regression model's estimates and inference. 




## 4.c Durbin Watson Test Linear Model 1 [raw data with outliers]
```{r chunk_name1325, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
durbinWatsonTest(lm1) 
```
p value pass



## 4.c Shapiro Wilks Test Linear Model 1 [raw data with outliers]
```{r}
## null is that the residuals are normally distributed --> residuals is the median distance of the point is from the regression line
residuals_lm1 <- residuals(lm1)

# Performing Shapiro-Wilk test for normality
shapiro.test(residuals_lm1)
```
```{r}
library(report)
report(lm1)
```


## 4.c Report Results Simple Linear Model 1 [raw data with outliers]

We fitted a linear model (estimated using OLS) to predict EPI2018Score with GDPpc (formula: EPI2018Score ~ GDPpc). The
model explains a statistically significant and substantial proportion of variance (R2 = 0.49, F(1, 178) = 169.66, p <
.001, adj. R2 = 0.49). The model's intercept, corresponding to GDPpc = 0, is at 48.11 (95% CI [46.26, 49.96], t(178) =
51.31, p < .001). Within this model:

  - The effect of GDPpc is statistically significant and positive (beta = 4.68e-04, 95% CI [3.97e-04, 5.39e-04], t(178)
= 13.03, p < .001; Std. beta = 0.70, 95% CI [0.59, 0.80])

Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence
Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.




## 4.c logging data 

```{r}
yaledat$log_GDPpc <- log(yaledat$`GDPpc`)
```
## 4.d Linear Model Logged Data
```{r}
lm2_logged <- lm(`EPI2018Score` ~ `log_GDPpc`, data = yaledat)


# Generate summary table
summary(lm2_logged)

# Extract coefficients table
coefficients_table2 <- lm2_logged$coefficients

# Load required library
library(knitr)

# Create a pretty table using kable
kable(coefficients_table2, format = "markdown")
```


## 4.d Equation for slope of logged data
$$\hat{y_{i}} = -23.28 + .086 × GDPpc$$
## 4.e interpertation for equation above

for every one percent  change in GDP per capita the EPISCORE increases by 0.086 units.
## 4.c scatterplot logged data
```{r}
yaledat %>% ggplot(aes(x = log_GDPpc, y = EPI2018Score)) + geom_point()

```
## 4.c Confidence interval Logged
```{r chunk_name1023,warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
# Compute confidence intervals
conf_intervals2 <- confint(lm2_logged, 'EPI2018Score', level = 0.95)

# Load required library
library(knitr)

# Create a pretty table using kable
kable(conf_intervals2, format = "markdown")

```
they have 97.5% and 2.5 percent confidence interval;
look how pretty that looks! 


## 4.c Plot  [Logged comparison with outliers for comparisson]
```{r chunkname68866886868, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
plot(lm2_logged)
```
Looking much more fitted after logged transformed, significantly reduced heteroskedasticity

## 4.c [Logged comparison with outliers for comparisson]
```{r chunk_name14, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
library(car)
library(lmtest)
ncvTest(lm2_logged)
bptest(lm2_logged, studentize = F)
bptest(lm2_logged, studentize = T)
```
owever, for Model 2, there is no significant evidence of heteroscedasticity, suggesting that the assumption of constant variance holds for this model.(p = .06), (p = .1) (p= .06) all above .05!
## 4.c Cooks Distnace
```{r chunk_name9999, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}

cooks.distance(lm2_logged) > 3*mean(cooks.distance(lm2_logged))
residuals8 <- cooks.distance(lm2_logged) > 3 * mean(cooks.distance(lm2_logged))
# Use logical indexing to filter out the true values
true_residuals8 <- cooks.distance(lm2_logged)[residuals8]

true_residuals8


```







## 4 [Logged comparison with outliers for comparisson]
```{r chunk_name25, warning=FALSE, fig.show='only', echo=FALSE, results='asis', message=FALSE}
library(car)
library(lmtest)
library(kableExtra)

durbinWatsonTest(lm2_logged) 

```


the p-value is 0.302, which is greater than 0.05. Therefore, there is insufficient evidence to reject the null hypothesis, suggesting that there is no significant autocorrelation in the residuals at lag 1.


In this case, the p-value is very small (p = .3 ). This suggests strong evidence against the null hypothesis of normality. Typically, if the p-value is more than a chosen significance level (such as 0.05), we fail to reject the null hypothesis. So, in this case, we would fail to reject the null hypothesis and conclude that the residuals are normally distributed. pass

```{r}
library(GGally)
theme_set(theme_bw())
ggpairs(yaledat[c(11,15)])
```


passed with much higher coorelation.



```{r}
library(performance)
check_model(lm2_logged)
```
## 4.f R^2 interpertation:
```{r}
summary(lm2_logged) 
```
R^2 value is 0.6657, which means that approximately 66.57% of the variance in the Environmental Performance Index (EPI) can be explained by the variation in the log of GDP per capita (GDPpc).


slope interpertation: 
for every one percent change in GDP per capita the EPISCORE increases by 0.086 units.

We fitted a linear model (estimated using OLS) to predict log_EPI2018Score with log_GDPpc (formula: log_EPI2018Score ~
log_GDPpc). The model explains a statistically significant and substantial proportion of variance (R2 = 0.66, F(1,
178) = 340.35, p < .001, adj. R2 = 0.65). The model's intercept, corresponding to log_GDPpc = 0, is at 2.54 (95% CI
[2.38, 2.70], t(178) = 31.66, p < .001). Within this model:

  - The effect of log GDPpc is statistically significant and positive (beta = 0.16, 95% CI [0.14, 0.18], t(178) = 18.45,
p < .001; Std. beta = 0.81, 95% CI [0.72, 0.90])

So it is clear that the linear correlation coefficient is statisticly significant showing that we can assume that the linear coorelation value of .8 is significant, assuming that GDP has significant influence on the change in EPIscore in the cooresponding data frame. We will reject the null hypothesis and accept the alternative hypothesis that GDP has an effect on EPISCore and changes EPIscore


## problem 5

```{r}
ozonedat <- read.csv("/Users/daniel/Downloads/BIO 591 Coding Files/Midterm/Midterm/ozone.data.csv")
```

## 5.b GGpairs
```{r}
ggpairs(ozonedat, columns = 1:4, title = "",  
  axisLabels = "show")
```


## 5.b first linear model
```{r}
lm3 <- lm(ozone ~ rad + temp + wind, data = ozonedat)
summary(lm3)
library(report)
report(lm3)
```




## 5.b vif first linear model
```{r}
vif(lm3)
```

Pass multicollinearity test so continue to cross validation


## 5.b GVLMA first linear model
```{r}
library(gvlma)
gvlma(lm3)
```
Initial GVLMA failed continue to cross validation


 
```{r}
library(knitr)

# Create data frame
data <- data.frame(
  "Global Stat" = 111.30248605,
  "Skewness" = 34.23929433,
  "Kurtosis" = 50.28629897,
  "Link Function" = 26.72320375,
  "Heteroscedasticity" = 0.05368901,
  "p-value" = c(0.000000e+00, 4.873491e-09, 1.328715e-12, 2.347851e-07, 8.167641e-01),
  "Decision" = c("Assumptions NOT satisfied!", "Assumptions NOT satisfied!", "Assumptions NOT satisfied!", "Assumptions NOT satisfied!", "Assumptions acceptable.")
)

# Display kable table
kable(data, align = "c", caption = "Assumption Test Results")

```




## 5.c Cross Validation and K fold

```{r, echo=FALSE, results='hide'}
library(gtools)

pastePerm<- function(row, names){
  keep<- which(row==1)
  if(length(keep)==0){
    return('1')
  }else{
    return(paste(names[keep],collapse='+'))
  }
}
my_sqrt <- function(var1){
  sqrt(var1)
}

dredgeform<- function(pred, covars, alwaysIn=''){
  p<- length(covars)
  perm.tab<- permutations(2, p, v=c(0,1), repeats.allowed=T)
  myforms<- NULL
  for(j in 1:nrow(perm.tab)){
    myforms[j]<- pastePerm(perm.tab[j,], covars)
  }
  myforms<- paste0(pred, '~ 1', alwaysIn,'+', myforms)
  return(myforms)
}

allformulas<- dredgeform(pred = "log(ozone)", covars = c("rad", "temp", "wind", "temp:wind","temp:rad","rad:wind"))
allformulas <- allformulas[2:length(allformulas)] #i dont want the intercept only one 

set.seed(123)
compare_var <- as.data.frame(matrix(ncol = 4, nrow = 0))
colnames(compare_var) <- c("formula", "RMSE", "R2", "MAE")

for ( i in 1:length(allformulas)) {
  
train.control <- trainControl(method = "repeatedcv", number = 3, repeats = 10)
# Train the full model
model_full <- train(as.formula(allformulas[i]), data = ozonedat, method = "lm",
               trControl = train.control)
# Summarize the results
compare_var[i, 1] <- allformulas[i]
compare_var[i, 2] <- mean(model_full$resample$RMSE)
compare_var[i, 3] <- mean(model_full$resample$Rsquared, na.rm = T)
compare_var[i, 4] <- mean(model_full$resample$MAE)


}

compare_var$prediction_error_rate <- compare_var$RMSE/mean(log(ozonedat$ozone)) 

compare_var %>% arrange(prediction_error_rate)

compare_var %>% arrange(RMSE)

```

## 5.c Cross Validation Results
```{r}
# Load necessary library
library(knitr)

# Create a data frame with the provided information
data2 <- data.frame(
  `Model Formula` = c(
    "log(ozone)~ 1+temp+wind+temp:wind+temp:rad",
    "log(ozone)~ 1+temp+temp:wind+temp:rad+rad:wind",
    "log(ozone)~ 1+temp+temp:wind+temp:rad",
    "log(ozone)~ 1+rad+temp+temp:wind",
    "log(ozone)~ 1+rad+temp+wind+temp:wind",
    "log(ozone)~ 1+temp+temp:rad+rad:wind",
    "log(ozone)~ 1+temp+wind+temp:rad+rad:wind",
    "log(ozone)~ 1+temp+wind+temp:rad",
    "log(ozone)~ 1+rad+temp+rad:wind",
    "log(ozone)~ 1+rad+temp+wind+temp:wind+rad:wind"
  ),
  `RMSE` = c(
    0.5127035, 0.5184218, 0.5194998, 0.5197990, 0.5203636,
    0.5209050, 0.5212435, 0.5253292, 0.5259307, 0.5260944
  ),
  `R2` = c(
    0.66488938, 0.65778198, 0.65869852, 0.64897106, 0.65858725,
    0.66490925, 0.66263490, 0.66394938, 0.66026420, 0.65146392
  ),
  `MAE` = c(
    0.4017731, 0.4018344, 0.4015029, 0.4013519, 0.4081567,
    0.4006607, 0.4041197, 0.4045672, 0.4068017, 0.4127110
  )
)

# Print the table using kable
kable(data2, format = "markdown")

```




## New Linear Model from Cross Validated Results [ with outliers]
```{r, echo=FALSE, results='hide'}
lm10 <- lm(log(ozone)~ 1+temp+wind+temp:wind+temp:rad, data = ozonedat,)

# Generate summary table
summary_table10 <- summary(lm10)
gvlma(lm10)

```
```{r}
library(kableExtra)

data3 <- data.frame(
  Test = c("Global Stat", "Skewness", "Kurtosis", "Link Function", "Heteroscedasticity"),
  `Global Stat` = c(27.919015, 3.464400, 14.078607, 1.690542, 8.685466),
  `p-value` = c("1.295329e-05", "6.270323e-02", "1.753266e-04", "1.935296e-01", "3.207576e-03"),
  Decision = c("Assumptions NOT satisfied!", "Assumptions acceptable.", "Assumptions NOT satisfied!", "Assumptions acceptable.", "Assumptions NOT satisfied!")
)

kable(data3, format = "html") %>%
  kable_styling()

```


```{r, echo=FALSE, results='hide'}
# Assuming lm1 is your linear model
residuals <- cooks.distance(lm10) > 3 * mean(cooks.distance(lm10))
# Use logical indexing to filter out the true values
true_residuals <- cooks.distance(lm10)[residuals]

true_residuals
```
## 5.Cooks Distance on Cross Validated Linear Model
```{r}
library(kableExtra)

data4 <- data.frame(
  Lag = c(11, 17, 18, 19, 20, 30, 45, 77, 85),
  Autocorrelation = c(0.03621747, 0.28468657, 0.03538612, 0.03627450, 0.04899801, 0.08359091, 0.04480581, 0.04071742, 0.03451473)
)

kable(data4, format = "html") %>%
  kable_styling()

```




## Removing Cook's Distance points from cross validated data
```{r}
ozonedat_withoutoutliers <- ozonedat[-c(  17, 18, 20,30,45,70,77,107),]
kable(head(ozonedat_withoutoutliers))
```



## New Linear Model of Cross Validated with NO Outliers
```{r}
lm11 <- lm(log(ozone)~ 1+temp+wind+temp:wind+temp:rad, data = ozonedat_withoutoutliers,)

# Generate summary table
summary_table10 <- summary(lm11)

```

```{r}
gvlma(lm11)
```



## 5.c and d GVLMA final Linear Model
```{r}
library(kableExtra)

data <- data.frame(
  Test = c("Global Stat", "Skewness", "Kurtosis", "Link Function", "Heteroscedasticity"),
  Value = c(4.9146022, 0.0375543, 0.8078563, 0.9631888, 3.1060028),
  p_value = c(0.29617261, 0.84634084, 0.36875479, 0.32638476, 0.07800418),
  Decision = c("Assumptions acceptable.", "Assumptions acceptable.", "Assumptions acceptable.", "Assumptions acceptable.", "Assumptions acceptable.")
)

kable(data, format = "html") %>%
  kable_styling()

```


## 5.c and d Summary and Values for semi-final 
```{r}
summary(lm11)
```
## 5.c exponentiate variables of summary
```{r}
# Original coefficient estimates
coefs <- c(
  `(Intercept)` = -9.362e-01,
  temp = 5.741e-02,
  wind = 5.489e-02,
  `temp:wind` = -1.502e-03,
  `temp:rad` = 3.217e-05
)

# Exponentiate the coefficient estimates
exp_coefs <- exp(coefs)

# Display the exponentiated coefficients
 finalresults <- (exp_coefs - 1)*100

 
 finalresults   %>%
  kbl() %>%
  kable_styling()
 
```

Every time temperature increases by one unit ozone increases by 5.9%. Everytime wind increases by one unit ozone increases 5.64 %. 
Every additional rise in unit temp decreases the positive/neg effect of wind on the dependent variable of ozone by -.15%.  every additional in temp decreases the positive/neg effect of solar radiation on dependent variable ofozone decreases by -0.00321%.




## Question 5.e 
```{r}
library(performance)
check_model(lm11)
```
## 5.e simple main effects plots
```{r}
library(car)
avPlots(lm11)

```



Every time temputure increases by one unit ozone increases by 5.9%, evertime wind increases by one unit ozone decreases by -.15 %,every additional in temp decreases the positive/neg effect of wind on dependent variable of  ozone decreases by -.15.  every additional in temp decreases the positive/neg effect of solar radiation on dependent variable of  ozone decreases by -0.00321%.


What I did 
From the cross validation and subsequent data wrangling we have determined several things:
- Running the cross validation with the interactive effects on the logged ozone we where able to obtain more accurate results from the CV, with that and after removing outliers from that data frame, all assumptions passed in the GVLMA.

From these results we determine that the most accurate predictor variables are temp and wind with the most prominenet interaction effects being temp:wind and temp:rad on the response variable of ozone.


## 5.e plot coefecients simple main effects and interaction effect plots
```{r}
library(ggstance)
library(jtools)

plot_coefs(lm11)
```

## 5.e Interaction plots for ineraction effects
```{r}
# Load required libraries
library(interactions)
library(ggplot2)  # For plotting
library(dplyr)     # For data manipulation using %>%

#as well as interact plot 
interact_plot(lm11, pred = temp, modx = rad, plot.points = TRUE, point.alpha = 0.2)

interact_plot(lm11, pred = temp, modx = wind, plot.points = TRUE, point.alpha = 0.2)

```


